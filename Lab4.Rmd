---
title: 'P&S-2025: Lab assignment 4'
author: "Hombosh Oleh, Matseliukh Maksym, Leshchuk Roman"
output:
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

# Setup

```{r}
team_id <- 39
set.seed(team_id)

library(ggplot2)
```

# Work Breakdown Structure

| Team Member  | Tasks Estimated Effort |
|--------------|------------------------|
| Hombosh Oleh | Problem 1, Problem 2   |
| Matseliukh Maksym     | Problem 3              |
| Leshchuk Roman     | Problem 4              |


# Data Generation

First, we generate the samples $X$ and $Y$ based on the team ID
($n = `r team_id`$). The formula for the sequence is
$a_k = \{ k \ln (k (2n + \pi)) \}$, where $\{x\}$ denotes the fractional
part of $x$.

```{r}
n <- team_id

get_ak <- function(k, n) {
  val <- k * log((k^2) * n + pi)
  return(val - floor(val))
}

k_x <- 1:100
k_y <- 1:50

a_x <- sapply(k_x, get_ak, n = n)
a_y <- sapply(k_y, function(l) get_ak(l + 100, n = n))

X <- qnorm(a_x)
Y <- qnorm(a_y)

df_X <- data.frame(Value = X, Group = "X")
df_Y <- data.frame(Value = Y, Group = "Y")
df_combined <- rbind(df_X, df_Y)

ggplot(df_combined, aes(x = Value, fill = Group)) +
  geom_density(alpha = 0.5) +
  theme_minimal() +
  labs(title = "Density of Generated Samples X and Y")


cat("Sample X (n=100): Mean =", mean(X), ", Variance =", var(X), "\n")
cat("Sample Y (n=50):  Mean =", mean(Y), ", Variance =", var(Y), "\n")
```

# Problem 1.

## Theoretical Background: Two-Sample Z-Test for Means

In this problem, we test whether the population means of two independent
samples, $X$ and $Y$, are equal. We assume that the population variances
are known and equal to 1.

**1. Hypotheses:** The null hypothesis ($H_0$) assumes no difference
between the means, while the alternative hypothesis ($H_1$) claims a
significant difference exists. $$
\begin{aligned}
H_0 &: \mu_1 = \mu_2 \\
H_1 &: \mu_1 \neq \mu_2
\end{aligned}
$$

**2. Test Statistic (Z):** Since the population variances
($\sigma_1^2, \sigma_2^2$) are known, we use the Z-statistic: $$
Z = \frac{\bar{X} - \bar{Y}}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}}
$$ Where: \* $\bar{X}, \bar{Y}$ are the sample means. \*
$\sigma_1^2 = \sigma_2^2 = 1$ are the known population variances. \*
$n_1, n_2$ are the sample sizes.

**3. Decision Rule:** For a significance level $\alpha = 0.05$, we
reject $H_0$ if the absolute value of the calculated Z-statistic exceeds
the critical value from the standard normal distribution:
$$|Z_{stat}| > Z_{1-\alpha/2} \approx 1.96$$

## Code

```{r}
sigma1 <- 1; sigma2 <- 1
n1 <- length(X); n2 <- length(Y)
mean_x <- mean(X); mean_y <- mean(Y)
se <- sqrt((sigma1^2 / n1) + (sigma2^2 / n2))
z_stat <- (mean_x - mean_y) / se
p_value_z <- 2 * (1 - pnorm(abs(z_stat)))

z_vals <- seq(-4, 4, length=1000)
z_dens <- dnorm(z_vals)
df_z <- data.frame(z=z_vals, density=z_dens)

p2 <- ggplot(df_z, aes(x=z, y=density)) +
  geom_line(size=1) +
  geom_area(data=subset(df_z, z < -1.96), aes(y=density), fill="red", alpha=0.4) +
  geom_area(data=subset(df_z, z > 1.96), aes(y=density), fill="red", alpha=0.4) +
  geom_vline(xintercept = z_stat, color="blue", linetype="dashed", size=1.2) +
  annotate("text", x=2.5, y=0.3, label="Rejection Region\n(alpha=0.05)", color="red", size=3.5) +
  annotate("text", x=z_stat, y=0.1, label=paste("Z-stat =", round(z_stat, 2)), 
           color="blue", angle=90, vjust=-0.5) +
  labs(title = "Z-test Hypothesis Visualization",
       subtitle = "Blue line: Calculated Z-statistic | Red area: Rejection Region",
       x = "Z-score", y = "Density") +
  theme_minimal()

print(p2)

cat("Z-statistic:", z_stat, "\n")
cat("P-value:", p_value_z, "\n")
```

## **Conclusion:**

Based on the obtained results: \* The Z-statistic is **1.548**. \* The
P-value is **0.122**.

Since the P-value ($0.122$) is **greater than** the significance level
($\alpha = 0.05$), we **fail to reject the null hypothesis** ($H_0$).

*Interpretation:* We do not have sufficient statistical evidence to
claim that the true population means $\mu_1$ and $\mu_2$ are different.
The observed difference between the sample means ($-0.053$ vs $-0.322$)
is likely due to random sampling variation rather than a real difference
in the populations.

# Problem 2

## Theoretical Background: F-Test for Equality of Variances

Here, we investigate whether the variance of the first population is
strictly greater than the variance of the second population.

**1. Hypotheses:** The null hypothesis states that the variances are
equal. The alternative hypothesis is one-sided (right-tailed). $$
\begin{aligned}
H_0 &: \sigma_1^2 = \sigma_2^2 \\
H_1 &: \sigma_1^2 > \sigma_2^2
\end{aligned}
$$

**2. Test Statistic (F):** The F-statistic is the ratio of the sample
variances ($S_X^2$ and $S_Y^2$): $$
F = \frac{S_X^2}{S_Y^2}
$$ Under the null hypothesis, this statistic follows an F-distribution
with degrees of freedom $df_1 = n_1 - 1$ and $df_2 = n_2 - 1$.

**3. Decision Rule:** For a significance level $\alpha = 0.05$, we
reject $H_0$ if the calculated F-statistic is greater than the critical
value: $$F_{stat} > F_{\alpha, n_1-1, n_2-1}$$ Where
$F_{\alpha, n_1-1, n_2-1}$ is the quantile of the F-distribution such
that the area to the right is $\alpha$.

## Code

```{r}
var_x <- var(X); var_y <- var(Y)
f_stat <- var_x / var_y
df1 <- n1 - 1; df2 <- n2 - 1
f_crit <- qf(0.95, df1, df2)
p_value_f <- 1 - pf(f_stat, df1, df2)

f_vals <- seq(0, max(3, f_stat + 1), length=1000)
f_dens <- df(f_vals, df1, df2)
df_f <- data.frame(f=f_vals, density=f_dens)

p3 <- ggplot(df_f, aes(x=f, y=density)) +
  geom_line(size=1) +
  geom_area(data=subset(df_f, f > f_crit), aes(y=density), fill="red", alpha=0.4) +
  geom_vline(xintercept = f_stat, color="blue", linetype="dashed", size=1.2) +
  annotate("text", x=f_crit + 0.5, y=0.4, label="Rejection Region\n(H1: Var1 > Var2)", color="red", size=3.5) +
  annotate("text", x=f_stat, y=0.2, label=paste("F-stat =", round(f_stat, 2)), 
           color="blue", angle=90, vjust=-0.5) +
  labs(title = "F-test Hypothesis Visualization",
       subtitle = paste("F-dist (df1=", df1, ", df2=", df2, ")"),
       x = "F-score", y = "Density") +
  theme_minimal()

print(p3)

cat("F-statistic:", f_stat, "\n")
cat("Critical Value:", f_crit, "\n")
cat("P-value:", p_value_f, "\n")
```

## Conclusion

**Conclusion:** Based on the obtained results: \* The F-statistic is
**0.750**. \* The P-value is **0.886**.

Since the P-value ($0.886$) is **greater than** the significance level
($\alpha = 0.05$) and the calculated F-statistic ($0.750$) is **less
than** the critical value ($1.531$), we **fail to reject the null
hypothesis** ($H_0$).

*Interpretation:* There is no statistical evidence to support the claim
that the variance of the first population is greater than the variance
of the second population ($H_1: \sigma_1^2 > \sigma_2^2$). In fact, the
sample variance of $X$ ($1.115$) turned out to be lower than that of $Y$
($1.487$), which contradicts the direction of the alternative
hypothesis.


# Problem 3

## Theoretical Background: Kolmogorov-Smirnov Test

The Kolmogorov-Smirnov (KS) test is a test used to compare distributions. It measures the maximum distance between two cumulative distribution functions (CDFs).

**Main Idea:**
The KS test compares:

1. **Empirical Cumulative Distribution Function (ECDF)** - a step function built directly from the sample data
2. **Theoretical Cumulative Distribution Function (CDF)** - the hypothesized distribution

**Test Statistic:**
$$
D = \sup_x |F_n(x) - F(x)|
$$
where $F_n(x)$ is the ECDF from the sample and $F(x)$ is the theoretical CDF.

**Types of KS Tests:**

- **One-sample KS test(also called Kolmogorov):** Compares a sample with a specified distribution
- **Two-sample KS test(also called Kolmogorov-Smirnov):** Compares two samples to see if they come from the same distribution

**Decision Rule:**
For significance level $\alpha = 0.05$:

- **Rejection region:** $D > D_{critical}$ (depends on sample size)
- **Decision:** Reject $H_0$ if p-value $< 0.05$


## (a): Testing Normality of $\{x_k\}_{k=1}^{100}$

**Hypotheses:**
$$
\begin{aligned}
H_0 &: \{x_k\}_{k=1}^{100} \text{ follows } N(\mu, \sigma^2) \text{ with parameters estimated from sample} \\
H_1 &: \{x_k\}_{k=1}^{100} \text{ does not follow Normal distribution}
\end{aligned}
$$

**Test Used:** One-sample Kolmogorov-Smirnov test

**Why:** The KS test compares the empirical distribution of our data with the theoretical normal distribution. We estimate the parameters ($\mu$ and $\sigma$) from the sample itself.

**Rejection Region:** For $\alpha = 0.05$, we reject $H_0$ if $D > D_{critical}$ or equivalently if p-value $< 0.05$.

### Code
```{r}
# Estimate parameters from the sample
mean_x <- mean(X)
sd_x <- sd(X)

# Perform KS test for normality
ks_test_a <- ks.test(X, "pnorm", mean = mean_x, sd = sd_x)

# Visualization
par(mfrow = c(1, 2))

# ECDF vs Theoretical CDF
plot(ecdf(X), main = "ECDF vs Normal CDF", 
     xlab = "x", ylab = "Cumulative Probability",
     col = "blue", lwd = 2, cex.main = 0.9)
curve(pnorm(x, mean = mean_x, sd = sd_x), 
      add = TRUE, col = "red", lwd = 2, lty = 2)
legend("topleft", c("Empirical (ECDF)", "Theoretical Normal"), 
       col = c("blue", "red"), lty = c(1, 2), lwd = 2, cex = 0.8)

# Histogram with theoretical density
hist(X, breaks = 30, probability = TRUE, 
     main = "Histogram vs Normal Density",
     xlab = "x", col = "lightblue", border = "white", cex.main = 0.9)
curve(dnorm(x, mean = mean_x, sd = sd_x), 
      add = TRUE, col = "red", lwd = 2)
legend("topright", "Normal density", col = "red", lwd = 2, cex = 0.8)

par(mfrow = c(1, 1))
```

### Statistics and Conclusion

**Sample Parameters:**

- Mean ($\hat{\mu}$) = `r round(mean_x, 6)`
- Standard deviation ($\hat{\sigma}$) = `r round(sd_x, 6)`

**Test Statistics:**

- D statistic = `r round(ks_test_a$statistic, 6)`
- P-value = `r round(ks_test_a$p.value, 6)`

**Conclusion:**

`r if(ks_test_a$p.value > 0.05) { paste0("Since the p-value (", round(ks_test_a$p.value, 6), ") is **greater than** the significance level (α = 0.05), we **fail to reject the null hypothesis** (H₀). The data is consistent with a normal distribution. This result is expected since the data was generated using qnorm (inverse normal CDF), which should produce normally distributed values.") } else { paste0("Since the p-value (", round(ks_test_a$p.value, 6), ") is **less than** the significance level (α = 0.05), we **reject the null hypothesis** (H₀). The data does not follow a normal distribution. This is surprising given that the data was generated via qnorm, suggesting that the fractional part sequence a_k may not be truly uniform.") }`

---

## (b): Testing Exponential Distribution of $\{|x_k|\}_{k=1}^{100}$

**Hypotheses:**
$$
\begin{aligned}
H_0 &: \{|x_k|\}_{k=1}^{100} \text{ follows Exponential}(\lambda = 1) \\
H_1 &: \{|x_k|\}_{k=1}^{100} \text{ does not follow Exponential}(\lambda = 1)
\end{aligned}
$$

**Test Used:** One-sample Kolmogorov-Smirnov test

**Why:** We want to test if the absolute values of our data follow an exponential distribution with rate parameter $\lambda = 1$. The exponential distribution has mean $1/\lambda = 1$.

**Rejection Region:** For $\alpha = 0.05$, we reject $H_0$ if $D > D_{critical}$ or equivalently if p-value $< 0.05$.

### Code
```{r}
# Take absolute values
abs_X <- abs(X)

# Perform KS test for exponential distribution
ks_test_b <- ks.test(abs_X, "pexp", rate = 1)

# Visualization
par(mfrow = c(1, 2))

# ECDF vs Theoretical CDF
plot(ecdf(abs_X), main = "ECDF vs Exponential CDF", 
     xlab = "|x|", ylab = "Cumulative Probability",
     col = "blue", lwd = 2, cex.main = 0.9)
curve(pexp(x, rate = 1), 
      add = TRUE, col = "red", lwd = 2, lty = 2)
legend("bottomright", c("Empirical (ECDF)", "Theoretical Exp(1)"), 
       col = c("blue", "red"), lty = c(1, 2), lwd = 2, cex = 0.8)

# Histogram with theoretical density
hist(abs_X, breaks = 30, probability = TRUE, 
     main = "Histogram vs Exponential Density",
     xlab = "|x|", col = "lightgreen", border = "white", cex.main = 0.9)
curve(dexp(x, rate = 1), 
      add = TRUE, col = "red", lwd = 2)
legend("topright", "Exp(1) density", col = "red", lwd = 2, cex = 0.8)

par(mfrow = c(1, 1))
```

### Statistics and Conclusion

**Sample Statistics of $\{|x_k|\}$:**

- Sample mean = `r round(mean(abs_X), 6)`
- Sample standard deviation = `r round(sd(abs_X), 6)`

**Theoretical Exponential(1) parameters:**

- Mean = $1/\lambda = 1$
- Standard deviation = $1/\lambda = 1$

**Test Statistics:**

- D statistic = `r round(ks_test_b$statistic, 6)`
- P-value = `r round(ks_test_b$p.value, 6)`

**Conclusion:**

Since the p-value (0.080208) is greater than the significance level (α = 0.05), we fail to reject the null hypothesis (H₀). The absolute values are consistent with Exponential(λ=1) distribution

---

## (c): Testing Equality of Distributions of $\{x_k\}$ and $\{y_l\}$

**Hypotheses:**
$$
\begin{aligned}
H_0 &: \{x_k\}_{k=1}^{100} \text{ and } \{y_l\}_{l=1}^{50} \text{ have the same distribution} \\
H_1 &: \{x_k\}_{k=1}^{100} \text{ and } \{y_l\}_{l=1}^{50} \text{ have different distributions}
\end{aligned}
$$

**Test Used:** Two-sample Kolmogorov-Smirnov test

**Why:** The two-sample KS test compares the empirical distributions of two independent samples to determine if they come from the same underlying distribution. This is a non-parametric test that is sensitive to differences in location, scale, and shape.

**Rejection Region:** For $\alpha = 0.05$, we reject $H_0$ if $D > D_{critical}$ or equivalently if p-value $< 0.05$.

### Code
```{r}
# Perform two-sample KS test
ks_test_c <- ks.test(X, Y)

# Visualization
par(mfrow = c(1, 2))

# Comparison of ECDFs
plot(ecdf(X), main = "Comparison of ECDFs", 
     xlab = "Value", ylab = "Cumulative Probability",
     col = "blue", lwd = 2, cex.main = 0.9)
plot(ecdf(Y), add = TRUE, col = "red", lwd = 2, lty = 2)
legend("topleft", c("X sample (n=100)", "Y sample (n=50)"), 
       col = c("blue", "red"), lty = c(1, 2), lwd = 2, cex = 0.8)

# Overlaid histograms
hist(X, breaks = 30, probability = TRUE, 
     main = "Overlaid Histograms",
     xlab = "Value", col = rgb(0, 0, 1, 0.3), border = "blue",
     xlim = range(c(X, Y)), cex.main = 0.9)
hist(Y, breaks = 20, probability = TRUE, 
     col = rgb(1, 0, 0, 0.3), border = "red", add = TRUE)
legend("topright", c("X sample", "Y sample"), 
       fill = c(rgb(0, 0, 1, 0.3), rgb(1, 0, 0, 0.3)), cex = 0.8)

par(mfrow = c(1, 1))
```

### Statistics and Conclusion

**Sample Sizes:**

- $n_1$ (X sample) = `r length(X)`
- $n_2$ (Y sample) = `r length(Y)`

**Sample Statistics:**

- X: mean = `r round(mean(X), 6)`, sd = `r round(sd(X), 6)`
- Y: mean = `r round(mean(Y), 6)`, sd = `r round(sd(Y), 6)`

**Test Statistics:**

- D statistic = `r round(ks_test_c$statistic, 6)`
- P-value = `r round(ks_test_c$p.value, 6)`

**Conclusion:**

`r if(ks_test_c$p.value > 0.05) { paste0("Since the p-value (", round(ks_test_c$p.value, 6), ") is **greater than** the significance level (α = 0.05), we **fail to reject the null hypothesis** (H₀). The two samples are consistent with having the same distribution. This makes sense because both X and Y were generated using the same process (qnorm transformation of fractional parts), just with different indices in the sequence a_k.") } else { paste0("Since the p-value (", round(ks_test_c$p.value, 6), ") is **less than** the significance level (α = 0.05), we **reject the null hypothesis** (H₀). The two samples have significantly different distributions. This suggests that the sequence a_k has different properties at indices 1-100 versus 101-150, possibly due to the specific mathematical form of k·ln(k²n + π).") }`

---

## Summary and Comments on KS Test Results

### Key Observations:



**1. Test 3(a) - Normality:**
The test examines whether the generation process successfully produced normally distributed data. Since we used qnorm (the inverse CDF of the standard normal), we expect the data to pass this test if the input sequence $a_k$ is sufficiently uniform.

**2. Test 3(b) - Exponential Distribution:**
This test is theoretically interesting. For a standard normal variable $X \sim N(0,1)$, the absolute value $|X|$ follows a half-normal distribution (also called folded normal), not an exponential distribution.

**3. Test 3(c) - Comparison of Two Samples:**
Both X and Y were generated using the same mathematical process but from different segments of the sequence. We expect the samples to have similar distributions.

# Problem 4. Regression of Marks on Study Time

In this task we fit a linear regression of marks (dependent variable) on
study time (independent variable).

## (a) Scatter plot of Marks vs Study Time

Firstly, let's read and display our data:

```{r}
df <- read.csv("data.csv")

p_scatter <- ggplot(df, aes(x = time_study, y = Marks)) +
  geom_point(size = 2) +
  theme_minimal() +
  labs(title = "Marks vs Study time",
       x = "Study time",
       y = "Marks")

print(p_scatter)
```

**Comments:**

On this plot we can observe that there is correlation between study time
and marks. For this reason, it makes sense to try fitting it in linear
regression model.

## (b) Fit linear regression model and derivation of the regression equation

**Model:**

We fit the simple linear regression model

$$\text{marks}_i = \beta_0 + \beta_1 \cdot \text{time_study}_i + \varepsilon_i,$$

where $\varepsilon_i$ are i.i.d. errors with mean 0 and variance
$\sigma^2$.

**Estimators:**

We can use two different ways to estimate intercept and slope of linear
regression, both will give us same results:

### Maximum likelihood estimation

The **likelihood function** is

$$
L(\beta_0,\beta_1,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}^n} \exp\Bigg(
-\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2
\Bigg).
$$

To maximize this, we take the log-likelihood:

$$
\ell = -\frac{n}{2} \log(2\pi\sigma^2)
- \frac{1}{2\sigma^2} \sum (y_i - \beta_0 - \beta_1 x_i)^2.
$$

Maximizing expression with respect to $\beta_0,\beta_1$ is equivalent to
minimizing

$$
S = \sum (y_i - \beta_0 - \beta_1 x_i)^2
$$

Giving the MLEs:

$$
\hat\beta_1 = \frac{\sum (x_i-\bar{x})(y_i-\bar{y})}{\sum (x_i-\bar{x})^2} = \frac{n \sum{x_i y_i} - \sum{x_i} \sum{y_i}}{n \sum x_i^2 - (\sum x_i)^2}, \qquad \hat\beta_0 = \bar{y} - \hat\beta_1\bar{x}.
$$

### Ordinary least squares (OLS) estimation

The OLS estimator minimizes the sum of squared residuals

$$
S(\beta_0,\beta_1)=\sum_{i=1}^n
(y_i-\beta_0-\beta_1 x_i)^2.
$$

To minimize, we set partial derivatives to zero:

$$
\frac{\partial S}{\partial \beta_0} = -2\sum_{i=1}^n (y_i-\beta_0-\beta_1 x_i)=0,
\frac{\partial S}{\partial \beta_1} = -2\sum_{i=1}^n x_i(y_i-\beta_0-\beta_1 x_i)=0.
$$

Solving gives us same results as maximum likelihood estimation

$$
\hat\beta_1=\frac{\sum (x_i-\bar{x})(y_i-\bar{y})}{\sum
(x_i-\bar{x})^2},\qquad \hat\beta_0=\bar{y}-\hat\beta_1\bar{x}
$$

Then we compute the regression model coefficients manually:

```{r}
x <- df$time_study
y <- df$Marks

slope_manual <- sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)
intercept_manual <- mean(y) - slope_manual * mean(x)

cat("Beta1 (slope):", slope_manual, "\n")
cat("Beta0 (intercept):", intercept_manual, "\n")
```

**Visualization:**

```{r}
p_scatter <- ggplot(df, aes(x = time_study, y = Marks)) +
  geom_point(size = 2) +
  geom_abline(intercept = intercept_manual, slope = slope_manual, color = "blue") +
  theme_minimal() +
  labs(title = "Marks vs Study time",
       x = "Study time",
       y = "Marks")

print(p_scatter)
```

**Comments:**

We estimated linear regression parameters for given data. Now, it is
important to know how good our model fits actual data, because not all
datasets are suitable for linear regression

## (c) Evaluate goodness-of-fit for the fitted line

We evaluate fit with coefficient of determination $R^2$. It is defined
as:

$$
SS_{tot} = \sum_{i=1}^n (y_i-\bar{y})^2,\\
SS_{res} = \sum_{i=1}^n (y_i-\hat{y}_i)^2,\\
R^2 = 1 - \frac{SS_{res}}{SS_{tot}}.
$$

$SS_{tot}$ part shows us how far is our sample y from mean y in average,
while $SS_{res}$ tells us how far is it in average from our predicted y
(we predict different y for each data entry from our sample). If we take
raw $SS_{res}$, it will not give us representative results, because
variance of y itself could be different. So if we divide $SS_{res}$ by
$SS_{tot}$, we want this value be as small as possible (that means that
our model gives smaller distance difference than when we simply take
sample mean as prediction for each data entry), so we subtract this
fraction from 1 to get our $R^2$ score:

```{r}
predicted_y = x * slope_manual + intercept_manual

SS_res <- sum((y - predicted_y)^2)

SS_tot <- sum((y - mean(y))^2)

R_squared_manual <- 1 - SS_res / SS_tot

cat("R^2:", R_squared_manual, "\n")
```

**Comments:**

As we can see, linear regression fits data pretty well: $R^2$ is almost
90%. It means that linear regression model can estimate and predict
marks based on study time well (if to consider our sample good enough)

## (d) Test whether study time is significant in predicting marks

**Idea:**

We test

$$H_0: \beta_1 = 0 \quad\text{vs}\quad H_1: \beta_1 \neq 0.$$

We will use t-test, because we do not know real variance of slope, but
we can estimate it using our sample variance. Let's calculate
t-statistic for the slope: it is how far our estimated slope is from the
null hypothesis value, measured in units of its standard error. We
calculate it dividing sample slope by its sample standard error (sample
standard error is calculated like standard deviation, but for sample -
so we are taking square root of sample variance). To calculate sample
variance of slope, we divide residual variance $\hat\sigma^2$ by spread
of x in sample:

$$t = \frac{\hat{\beta}_1 - 0}{\widehat{SE}(\hat{\beta}_1)} = \frac{\hat{\beta}_1}{\sqrt{Var(\hat{\beta}_1)}} = \frac{\hat{\beta}_1}{\sqrt{\frac{\hat\sigma^2}{\sum (x_i - \bar x)^2 }}} = \frac{\hat{\beta}_1}{\sqrt{\frac{\sum (y_i - \bar y)^2}{(n - 2) \sum (x_i - \bar x)^2 }}}$$

We subtract 0 in numerator - value of slope under $H_0$, and dividing by
$n - 2$ while finding residual variance, because there are two
parameters (intercept and slope), which take one degree of freedom each.

In this way, we find t-statistic, which follows a Student's
t-distribution with $n-2$ degrees of freedom. Then, p-value is just a
probability of observing a t-statistic at least as extreme as the one we
computed, assuming that null hypothesis is true. To calculate this
probability, we are finding probability of getting t-statistic out of
interval [-t, t], and compute this probability using CDF of Student's
distribution:

```{r}
sample_error_variance <- sum((y - predicted_y)^2) / (length(y) - 2)

sample_slope_variance <- sample_error_variance / sum((x - mean(x))^2)

t_stat <- slope_manual / sqrt(sample_slope_variance)

cat("T-statistic: ", t_stat, "\n")

p_value <- 2 * (1 - pt(abs(t_stat), df=length(y)-2))

cat("P-value: ", p_value, "\n")
```

**Comments:**

Because we got very big t-statistic and very small p-value, we can
safely reject null hypothesis: it is very unlikely to get data with
slope at least as far from 0 as we got if null hypothesis is true, so
null hypothesis probably should be rejected (with huge probability).

## (e) Prediction for student who studies 8 hours

Firstly, we can compute the point prediction, just fitting student's
data into our regression model with known slope and intercept:

```{r}
study_time = 8.0

predicted_score = study_time * slope_manual + intercept_manual

cat("Score predicted by regression model for time", study_time, ":", predicted_score)
```

**Comments:**

Using our regression model, we can predict student's score based on time
he/she studied. We can predict score which is most probable, and it
probably would be close enough, because we already tested that our model
has big goodness-of-fit.

## (f) Suggestions to potentially improve prediction accuracy

1.  **Add relevant predictors.** Study time alone explains only part of
    the variability. Including additional features such as attendance,
    homework completion, sleep, or difficulty of course would likely
    improve accuracy.

2.  **Use nonlinear models or transformations.** If the relationship is
    not strictly linear, it could be helpful to try to add more
    parameters to our regression (now it has only intercept and slope),
    for example, parameter which will be multiplied by $time^2$.

3.  **Collect more data.** Current dataset consists of only 100 entries,
    which on practice is not enough for reliable prediction. Due to
    randomness of sample with such size or bias (for example, if it is
    data from only one university), our model could be not as accurate
    as we would like it to be for future predictions.

## Conclusion

In this task, we explored how to fit, analyse, and interpret a simple
linear regression model of marks on study time.

We began by visualizing the data and observed a clear positive
relationship between study time and marks. Using both OLS and MLE
approaches, we derived the regression line coefficients and confirmed
that the estimated slope and intercept are identical under both methods.

Goodness-of-fit analysis showed a high $R^2$, indicating that linear
regression model fits our data well. Hypothesis testing confirmed that
the slope is statistically significant: the large t-statistic and
extremely small p-value strongly reject the null hypothesis that study
time has no effect on marks.

We then used the model to predict the expected mark for a student who
studies 8 hours, demonstrating that model can be used to predict score
from time based on coefficients it calculated earlier.

Overall, the linear regression model is simple, but still powerful tool
for cases where there is linear correlation between parameters and we
need to predict one parameter while having others.
